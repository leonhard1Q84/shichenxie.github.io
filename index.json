[
{
	"uri": "/",
	"title": "",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "/post/dt-pd/",
	"title": "data.table 与 pandas",
	"tags": ["R", "python"],
	"categories": [],
	"series": [],
	"description": "",
	"content": " 常见的数据分析项目过程包括加载数据-清洗数据-(特征处理、可视化、模型模拟)-交流成果。其中，清洗数据与特征处理一般占据整个项目的大部分时间。熟练掌握相关工具，提高数据处理的效率，是开展数据分析工作的基础。 数据框（data frame）是我们接触最多的数据形式，其每一列都是向量且长度一致。在 R 中我通常使用 data.table 包对数据框进行处理分析，而在 python 环境中 pandas 最为常用的。根据 Database-like ops benchmark 显示，data.table 在大部分任务中性能表现最好，而且其语法也相对简洁统一。为了方便大家对比学习，本文将分别用 data.table 与 pandas 实现常见的数据分析任务12。\n探索数据 读取 csv 文件 library(data.table) packageVersion(\u0026#39;data.table\u0026#39;) ## [1] \u0026#39;1.12.8\u0026#39; url = \u0026quot;https://vincentarelbundock.github.io/Rdatasets/csv/datasets/HairEyeColor.csv\u0026quot; dt = fread(url)  import pandas as pd pd.__version__ ## \u0026#39;0.25.3\u0026#39; url = \u0026quot;https://vincentarelbundock.github.io/Rdatasets/csv/datasets/HairEyeColor.csv\u0026quot; df = pd.read_csv(url)    查看数据结构 #数据类型 class(dt) str(dt) # 列名 names(dt) # 打印前后几行 head(dt, n=3) tail(dt, n=3) # 维度 dim(dt) nrow(dt) ncol(dt) # 汇总 summary(dt)  #数据类型 type(df) df.dtypes # 列名 list(df) # 打印前后几行 df.head(n=3) df.tail(n=3) # 维度 df.shape len(df.index) len(df.columns) # 汇总 df.describe()     行选择与排序 rows filter # 基于行所在位置筛选 dt[c(3,1,2)] # 单条件筛选 dt[Hair == \u0026#39;Red\u0026#39;] # 多条件筛选 dt[Hair == \u0026#39;Black\u0026#39; \u0026amp; Freq \u0026gt;= 10 \u0026amp; Eye %in% c(\u0026#39;Brown\u0026#39;, \u0026#39;Blue\u0026#39;)]   # 基于行所在位置筛选 df.iloc[[2,0,1]] # python序数从0开始，2代表第三行 df.loc[[2,0,1]] # 如果index未修改，效果与iloc的一致 # 单条件筛选，去掉.loc效果一致 df.loc[df[\u0026#39;Hair\u0026#39;] == \u0026#39;Red\u0026#39;] # pandas 多条件筛选时要用|, \u0026amp;,~分别代表or,and,not; 且每个条件用括号区分 df.loc[(df[\u0026#39;Hair\u0026#39;] == \u0026#39;Black\u0026#39;) \u0026amp; (df[\u0026#39;Freq\u0026#39;] \u0026gt;= 10) \u0026amp; (df[\u0026#39;Eye\u0026#39;].isin([\u0026#39;Brown\u0026#39;, \u0026#39;Blue\u0026#39;]))]   rows arrange dt[order(Sex, -Freq)]  df.sort_values([\u0026#39;Sex\u0026#39;, \u0026#39;Freq\u0026#39;], ascending=[True, False])     列选择、新建与计算 columns select dt[,.(Hair, Freq)] # or dt[,c(\u0026#39;Eye\u0026#39;, \u0026#39;Sex\u0026#39;), with=FALSE]  df[[\u0026#39;Hair\u0026#39;, \u0026#39;Freq\u0026#39;]] # or df.loc[:,[\u0026#39;Eye\u0026#39;, \u0026#39;Sex\u0026#39;]] #选一列时也要保留[]，否则与df.Eye一样为series    columns mutate # 新建一列 dt[, nc := .I] # .I .N .SD为特殊符号,查看帮助?`.I` dt[,\u0026#39;nc0\u0026#39;] = 1:32 # 新建多列 dt[, `:=`( nc1 = 1:32, nc2 = paste(Hair, Eye, sep=\u0026#39;,\u0026#39;) )] # 基于条件新建列 dt[, nc3 := ifelse(Freq \u0026gt;= 10, 1, 0)] dt[Freq \u0026gt;= 20, nc4 := 2] # 删除列 dt[, nc := NULL] dt[, (c(\u0026#39;nc0\u0026#39;,\u0026#39;nc1\u0026#39;,\u0026#39;nc2\u0026#39;,\u0026#39;nc3\u0026#39;,\u0026#39;nc4\u0026#39;)) := NULL]  # 新建一列 df = df.assign(nc = pd.Series(range(32))) df.loc[:,\u0026#39;nc0\u0026#39;] = pd.Series(range(32), index=df.index) # 新建多列 df = df.assign( nc1 = pd.Series(range(32)), nc2 = df.Hair + \u0026#39;,\u0026#39; + df.Eye ) # 基于条件新建列 df = df.assign(nc3 = df.apply(lambda x: 1 if x.Freq \u0026gt;= 10 else 0, axis=1)) df.loc[df.Freq \u0026gt;= 20, \u0026#39;nc4\u0026#39;] = 2 # 删除列 df = df.drop(\u0026#39;nc\u0026#39;, axis=1) df.drop([\u0026#39;nc0\u0026#39;,\u0026#39;nc1\u0026#39;,\u0026#39;nc2\u0026#39;,\u0026#39;nc3\u0026#39;,\u0026#39;nc4\u0026#39;], axis=1, inplace=True)    columns summarise # 对一列进行计算 dt[, unique(Eye)] dt[, table(Eye)] dt[, max(Freq)] # 对多列进行计算 dt[, lapply(.SD, max)] # 所有列的最大值 dt[, lapply(.SD, function(x) mean(is.na(x)))] # 所有列的缺失率  # 基于列的计算 df.Eye.unique() df.Eye.value_counts() df.Freq.max() # 对多列进行计算 df.max() # 所有列的最大值 df.isnull().mean() # 所有列的缺失率     分组汇总计算 grouped summarise # max freq dt[, .(freq_max = max(Freq)), by = \u0026#39;Sex\u0026#39;] # count dt[, .(freq_count = .N), keyby = c(\u0026#39;Hair\u0026#39;, \u0026#39;Sex\u0026#39;)]  # max freq # pandas的groupby会自动删除缺失变量 df.groupby(\u0026#39;Sex\u0026#39;).agg({\u0026#39;Freq\u0026#39;:\u0026#39;max\u0026#39;}). rename(columns={\u0026#39;Freq\u0026#39;:\u0026#39;freq_max\u0026#39;}). reset_index() # count df.groupby([\u0026#39;Hair\u0026#39;,\u0026#39;Sex\u0026#39;]).agg({\u0026#39;Freq\u0026#39;:\u0026#39;count\u0026#39;}). rename(columns={\u0026#39;Freq\u0026#39;:\u0026#39;freq_count\u0026#39;}). reset_index()    grouped mutate \u0026amp; filter # 基于sex分组，新建一列等于freq的最大值 dt[, freq_max := max(Freq), by = \u0026#39;Sex\u0026#39;] # 基于sex分组，选取freq最大的行 dt[order(Freq)][, .SD[.N], by = \u0026#39;Sex\u0026#39;][]  # 基于sex分组，新建一列等于freq的最大值 df.loc[:,\u0026#39;freq_max\u0026#39;] = df.groupby(\u0026#39;Sex\u0026#39;)[\u0026#39;Freq\u0026#39;].transform(max) # 基于sex分组，选取freq最大的行 df.sort_values(\u0026#39;Freq\u0026#39;).groupby(\u0026#39;Sex\u0026#39;).tail(1)     行列转换 长宽表转换 # 长表转宽表 dt_w = dcast(dt, Hair+Sex~Eye, value.var = \u0026#39;Freq\u0026#39;, fun.aggregate = sum) # 宽表转长表 dt_l = melt(dt_w, id = c(\u0026#39;Hair\u0026#39;,\u0026#39;Sex\u0026#39;), variable.name = \u0026#39;Eye\u0026#39;, value.name = \u0026#39;Freq\u0026#39;)  # 长表转宽表 df_w = pd.pivot_table(df, index=[\u0026#39;Hair\u0026#39;,\u0026#39;Sex\u0026#39;], columns=\u0026#39;Eye\u0026#39;, values=\u0026#39;Freq\u0026#39;, aggfunc = sum).reset_index() # 宽表转长表 df_l = df_w.melt(id_vars = [\u0026#39;Hair\u0026#39;,\u0026#39;Sex\u0026#39;], var_name=\u0026#39;Freq\u0026#39;)    行列合并切割 # 一行切割为多行 dtr = dt[, paste0(Eye, collapse = \u0026#39;,\u0026#39;), keyby = c(\u0026#39;Hair\u0026#39;, \u0026#39;Sex\u0026#39;)] dtr[, .(Eye = unlist(strsplit(V1, \u0026#39;,\u0026#39;))), by = c(\u0026#39;Hair\u0026#39;, \u0026#39;Sex\u0026#39;)] # 一列切割为多列 dtc = dt[, .(Hair, eye_sex = paste(Eye, Sex, sep = \u0026#39;,\u0026#39;))] dtc[, c(\u0026#39;Eye\u0026#39;, \u0026#39;Sex\u0026#39;) := tstrsplit(eye_sex, \u0026#39;,\u0026#39;)]  # 一行切割为多行 dfr = df.groupby([\u0026#39;Hair\u0026#39;,\u0026#39;Sex\u0026#39;])[\u0026#39;Eye\u0026#39;].apply(lambda x: \u0026#39;,\u0026#39;.join(x)).reset_index() dfr.assign(Eye = dfr[\u0026#39;Eye\u0026#39;].str.split(\u0026#39;,\u0026#39;)).explode(\u0026#39;Eye\u0026#39;) # 一列切割为多列 dfc = df[[\u0026#39;Hair\u0026#39;]].assign(eye_sex = df.Eye+\u0026#39;,\u0026#39;+df.Sex) dfc[\u0026#39;Eye\u0026#39;], dfc[\u0026#39;Sex\u0026#39;]= dfc[\u0026#39;eye_sex\u0026#39;].str.split(\u0026#39;,\u0026#39;, 1).str       .column-left{ float: left; width: 49.2%; text-align: left; } .column-right{ float: right; width: 49.2%; text-align: left; } .row:after { content: \"\"; display: table; clear: both; }   .col2 { columns: 2 200px; /* number of columns and width in pixels*/ -webkit-columns: 2 200px; /* chrome, safari */ -moz-columns: 2 200px; /* firefox */ } .col3 { columns: 3 100px; -webkit-columns: 3 100px; -moz-columns: 3 100px; }     Data Manipulation with Python Pandas and R Data.Table↩\n R for Data Science↩\n   "
},
{
	"uri": "/post/",
	"title": "Post",
	"tags": ["index"],
	"categories": [],
	"series": [],
	"description": "Post page",
	"content": ""
},
{
	"uri": "/tags/python/",
	"title": "python",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/r/",
	"title": "R",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "/archive/",
	"title": "Archive",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "Archive Page",
	"content": ""
},
{
	"uri": "/post/fintech_credit/",
	"title": "[转]互联网金融的大数据风控",
	"tags": ["credit"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "大数据能够进行数据变现的商业模式目前主要包括，一是精准营销，典型的场景是商品推荐和精准广告投放，二是大数据风控，典型的场景是互联网金融的大数据风控。^[来源: 知乎]\n金融的本质是风险管理，风控是所有金融业务的核心。典型的金融借贷业务例如抵押贷款、消费贷款、供应链金融、以及票据融资都需要数据风控识别欺诈用户及评估用户信用等级。\n传统金融的风控主要利用了信用属性强大的金融数据，一般采用20个纬度左右的数据，利用评分来识别客户的还款能力和还款意愿。信用相关程度强的数据纬度为十个左右，包含年龄、职业、收入、学历、工作单位、借贷情况、房产，汽车、单位、还贷记录等，金融企业参考用户提交的数据进行打分，最后得到申请人的信用评分，依据评分来决定是否贷款以及贷款额度。其他同信用相关的数据还有区域、产品、理财方式、行业、缴款方式、缴款记录、金额、时间、频率等。\n互联网金融的大数据风控并不是完全改变传统风控，实际是丰富传统风控的数据纬度。互联网风控中，首先还是利用信用属性强的金融数据，判断借款人的还款能力和还款意愿，然后在利用信用属性较弱的行为数据进行补充，一般是利用数据的关联分析来判断借款人的信用情况，借助数据模型来揭示某些行为特征和信用风险之间的关系。\n互联网金融公司利用大数据进行风控时，都是利用多维度数据来识别借款人风险。同信用相关的数据越多地被用于借款人风险评估，借款人的信用风险就被揭示的更充分，信用评分就会更加客观，接近借款人实际风险。常用的互联网金融大数据风控方式有以下几种：\n 欺诈识别: 身份验证、线上申请信息与行为、黑名单与灰名单、移动设备信息 信用风险: 消费记录、社会关系、社会属性与行为、司法信息  一、验证借款人身份 验证借款人身份的五因素认证是姓名、手机号、身份证号、银行卡号、家庭地址。企业可以借助国政通的数据来验证姓名、身份证号，借助银联数据来验证银行卡号和姓名，利用运营商数据来验证手机号、姓名、身份证号、家庭住址。\n如果借款人是欺诈用户，这五个信息都可以买到。这个时候就需要进行人脸识别了，人脸识别等原理是调用国政通/公安局API接口，将申请人实时拍摄的照片/视频同客户预留在公安的身份证进行识别，通过人脸识别技术验证申请人是否是借款人本人。\n其他的验证客户的方式包括让客户出示其他银行的信用卡及刷卡记录，或者验证客户的学历证书和身份认证。\n二、分析提交的信息来识别欺诈 大部分的贷款申请都从线下移到了线上，特别是在互联网金融领域，消费贷和学生贷都是以线上申请为主的。\n线上申请时，申请人会按照贷款公司的要求填写多维度信息例如户籍地址，居住地址，工作单位，单位电话，单位名称等。如果是欺诈用户，其填写的信息往往会出现一些规律，企业可根据异常填写记录来识别欺诈。例如填写不同城市居住小区名字相同、填写的不同城市，不同单位的电话相同、不同单位的地址街道相同、单位名称相同、甚至居住的楼层和号码都相同。还有一些填写假的小区、地址和单位名称以及电话等。\n如果企业发现一些重复的信息和电话号码，申请人欺诈的可能性就会很高。\n三、分析客户线上申请行为来识别欺诈 欺诈用户往往事先准备好用户基本信息，在申请过程中，快速进行填写，批量作业，在多家网站进行申请，通过提高申请量来获得更多的贷款。\n企业可以借助于SDK或JS来采集申请人在各个环节的行为，计算客户阅读条款的时间，填写信息的时间，申请贷款的时间等，如果这些申请时间大大小于正常客户申请时间，例如填写地址信息小于2秒，阅读条款少于3秒钟，申请贷款低于20秒等。用户申请的时间也很关键，一般晚上11点以后申请贷款的申请人，欺诈比例和违约比例较高。\n这些异常申请行为可能揭示申请人具有欺诈倾向，企业可以结合其他的信息来判断客户是否为欺诈用户。\n四、利用黑名单和灰名单识别风险 互联网金融公司面临的主要风险为恶意欺诈，70%左右的信贷损失来源于申请人的恶意欺诈。客户逾期或者违约贷款中至少有30%左右可以收回，另外的一些可以通过催收公司进行催收，M2逾期的回收率在20%左右。\n市场上有近百家的公司从事个人征信相关工作，其主要的商业模式是反欺诈识别，灰名单识别，以及客户征信评分。反欺诈识别中，重要的一个参考就是黑名单，市场上领先的大数据风控公司拥有将近1000万左右的黑名单，大部分黑名单是过去十多年积累下来的老赖名单，真正有价值的黑名单在两百万左右。\n黑名单来源于民间借贷、线上P2P、信用卡公司、小额借贷等公司的历史违约用户，其中很大一部分不再有借贷行为，参考价值有限。另外一个主要来源是催收公司，催收的成功率一般小于于30%(M3以上的)，会产生很多黑名单。灰名单是逾期但是还没有达到违约的客户(逾期少于3个月的客户)，灰名单也还意味着多头借贷，申请人在多个贷款平台进行借贷。总借款数目远远超过其还款能力。\n黑名单和灰名单是很好的风控方式，但是各个征信公司所拥有的名单仅仅是市场总量的一部分，很多互联网金融公司不得不接入多个风控公司，来获得更多的黑名单来提高查得率。央行和上海经信委正在联合多家互联网金融公司建立统一的黑名单平台，但是很多互联网金融公司都不太愿意贡献自家的黑名单，这些黑名单是用真金白银换来的教训。另外如果让外界知道了自家平台黑名单的数量，会影响其公司声誉，降低公司估值，并令投资者质疑其平台的风控水平。\n五、利用移动设备数据识别欺诈行为 数据中一个比较特殊的就是移动设备数据反欺诈，公司可以利用移动设备的位置信息来验证客户提交的工作地和生活地是否真实，另外来可以根据设备安装的应用活跃来识别多头借贷风险。\n欺诈用户一般会使用模拟器进行贷款申请，移动大数据可以识别出贷款人是否使用模拟器。欺诈用户也有一些典型特征，例如很多设备聚集在一个区域，一起申请贷款。欺诈设备不安装生活和工具用App，仅仅安装和贷款有关的App，可能还安装了一些密码破译软件或者其他的恶意软件。\n欺诈用户还有可能不停更换SIM卡和手机，利用SIM卡和手机绑定时间和频次可以识别出部分欺诈用户。另外欺诈用户也会购买一些已经淘汰的手机，其机器上面的操作系统已经过时很久，所安装的App版本都很旧。这些特征可以识别出一些欺诈用户。\n六、利用消费记录来进行评分 大数据风控除了可以识别出坏人，还可以评估贷款人的还款能力。过去传统金融依据借款人的收入来判断其还款能力，但是有些客户拥有工资以外的收入，例如投资收入、顾问咨询收入等。另外一些客户可能从父母、伴侣、朋友那里获得其他的财政支持，拥有较高的支付能力。\n按照传统金融的做法，在家不工作照顾家庭的主妇可能还款能力较弱。无法给其提供贷款，但是其丈夫收入很高，家庭日常支出由其太太做主。这种情况，就需要消费数据来证明其还款能力了。\n常用的消费记录由银行卡消费、电商购物、公共事业费记录、大宗商品消费等。还可以参考航空记录、手机话费、特殊会员消费等方式。例如头等舱乘坐次数，物业费高低、高尔夫球俱乐部消费，游艇俱乐部会员费用，奢侈品会员，豪车4S店消费记录等消费数据可以作为其信用评分重要参考。\n互联网金融的主要客户是屌丝，其电商消费记录、旅游消费记录、以及加油消费记录都可以作为评估其信用的依据。有的互联金融公司专门从事个人电商消费数据分析，只要客户授权其登陆电商网站，其可以借助于工具将客户历史消费数据全部抓取并进行汇总和评分。\n七、参考社会关系来评估信用情况 物以类聚，人与群分。一般情况下，信用好的人，他的朋友信用也很好。信用不好的人，他的朋友的信用分也很低。\n参考借款人常联系的朋友信用评分可以评价借款人的信用情况，一般会采用经常打电话的朋友作为样本，评估经常联系的几个人(不超过6六个人)的信用评分，去掉一个最高分，去掉一个最低分，取其中的平均值来判断借款人的信用。这种方式挑战很大，只是依靠手机号码来判断个人信用可信度不高。一般仅仅用于反欺诈识别，利用其经常通话的手机号在黑名单库里面进行匹配，如果命中，则此申请人的风险较高，需要进一步进行调查。\n八、参考借款人社会属性和行为来评估信用 参考过去互联网金融风控的经验发现，拥有伴侣和子女的借款人，其贷款违约率较低;年龄大的人比年龄低的人贷款违约率要高，其中50岁左右的贷款人违约率最高，30岁左右的人违约率最低。贷款用于家庭消费和教育的贷款人，其贷款违约率低;声明月收入超过3万的人比声明月收入低于1万5千的人贷款违约率高;贷款次数多的人，其贷款违约率低于第一次贷款的人。\n经常不交公共事业费和物业费的人，其贷款违约率较高。经常换工作，收入不稳定的人贷款违约率较高。经常参加社会公益活动的人，成为各种组织会员的人，其贷款违约率低。经常更换手机号码的人贷款违约率比一直使用一个电话号码的人高很多。\n午夜经常上网，很晚发微博，生活不规律，经常在各个城市跑的申请人，其带贷款违约率比其他人高30%。刻意隐瞒自己过去经历和联系方式，填写简单信息的人，比信息填写丰富的人违约概率高20%。借款时间长的人比借款时间短短人，逾期和违约概率高20%左右。拥有汽车的贷款人比没有汽车的贷款人，贷款违约率低10%左右。\n九、利用司法信息评估风险 涉毒涉赌以及涉嫌治安处罚的人，其信用情况不是太好，特别是涉赌和涉毒人员，这些人是高风险人群，一旦获得贷款，其贷款用途不可控，贷款有可能不会得到偿还。\n寻找这些涉毒涉赌的嫌疑人，可以利用当地的公安数据，但是难度较大。也可以采用移动设备的位置信息来进行一定程度的识别。如果设备经常在半夜出现在赌博场所或赌博区域例如澳门，其申请人涉赌的风险就较高。另外中国有些特定的地区，当地的有一部分人群从事涉赌或涉赌行业，一旦申请人填写的居住地址或者移动设备位置信息涉及这些区域，也要引起重视。涉赌和涉毒的人员工作一般也不太稳定或者没有固定工作收入，如果申请人经常换工作或者经常在某一个阶段没有收入，这种情况需要引起重视。涉赌和涉毒的人活动规律比较特殊，经常半夜在外面活动，另外也经常住本地宾馆，这些信息都可以参考移动大数据进行识别。\n总之，互联网金融的大数据风控采用了用户社会行为和社会属性数据，在一定程度上补充了传统风控数据维度不足的缺点，能够更加全面识别出欺诈客户，评价客户的风险水平。互联网金融企业通过分析申请人的社会行为数据来控制信用风险，将资金借给合格贷款人，保证资金的安全。\n"
},
{
	"uri": "/tags/credit/",
	"title": "credit",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "/post/scorecard/",
	"title": "使用 R 语言开发评分卡模型",
	"tags": ["R", "credit"],
	"categories": [],
	"series": [],
	"description": "",
	"content": " 为了提高评分卡模型的开发效率，我为 R 语言社区贡献了一个开源项目 scorecard 包 (HomePage, Github, CRAN)。该 R 包提供了评分卡开发过程中的常用功能，包括变量粗筛、分箱与 woe 转换、模型评估、评分刻度转换等。\n评分卡模型的开发流程通常包括以下五个主要步骤：数据准备、WOE 分箱、模型拟合、模型评估、评分卡刻度。下面结合 scorecard 包完成一个简单的评分卡模型开发案例。更加详细的评分卡模型开发介绍请参考幻灯片。\n数据准备 首先加载 scorecard 包，并载入包内自带的德国信贷数据集。该数据集包含了1000个借款人的信贷数据，20个 X 特征与1个 Y 值。其详细信息参见 UCI 的德国信贷数据集网站。\nlibrary(scorecard) # load germancredit data data(germancredit) 载入数据集后，可先通过变量的 IV 值、缺失率以及单类别率对 X 特征进行初步筛选。var_filter 函数默认删除信息值小于0.02、缺失率大于95%或单类别比例大于95%的变量。var_filter 函数还能够人为设定需要删除或保留的变量，以及够返回变量删除的原因列表。\n# filter variable via missing rate, iv, identical rate dt = var_filter(germancredit, y = \u0026#39;creditability\u0026#39;) ## [INFO] filtering variables ... 将经过初筛的数据集拆分为训练集与测试集。在 split_df 函数中如果指定了 y 变量，那么将基于 y 变量分层拆分，如果没有指定，则随机拆分数据集。ratio 为拆分后两个数据集的样本量占比。 seed 为随机种子，用于重现拆分的样本。\n# breaking dt into train and test dt_list = split_df(dt, y=\u0026quot;creditability\u0026quot;, ratio = 0.6, seed = 30) label_list = lapply(dt_list, function(x) x$creditability)  WOE分箱 接下来对数据集进行分箱与 woe 值转换。由于这个数据集样本量比较小，我们可以直接对全样本进行分箱处理。如果数据量允许，应该使用训练集进行分箱，并使用训练集得到的 woe 值对其他数据集进行woe替换。\nwoebin 函数提供了树形 tree、卡方合并 chimerge、等宽 width 与等高 freq 四种分箱方法。以树形分箱为例，默认情形是当信息值增益率 stop_limit 小于0.1, 或分箱数 bin_num_limit 大于8(缺失值除外)时停止分箱，同时确保每一个分箱的样本占比 count_distr_limit 不小于5%。当然还能够通过 breaks_list 手动设定分箱节点。\nwoebin 函数输出的结果为多个 data.frame 组成的 list，可通过 data.table::rbindlist 或 dplyr::bind_rows 函数合并为一个数据框然后保存。当然 woebin 函数也能够直接输出一个由分箱切割点组成的 list 并保存，下次使用时直接通过 woebin 函数对 breaks_list 参数对新的数据集进行分箱。\nwoebin_adj 函数可逐个观察每个变量的分箱情况。如果不满意默认的分箱结果，可以手动修改。最终返回一个经过手动调整的分箱节点。\n分箱之后，需要使用 woebin_ply 函数将训练集与测试集转换为对应的 woe 值。\n# woe binning bins = woebin(dt, \u0026quot;creditability\u0026quot;, print_step=0) ## [INFO] creating woe binning ... bins[[12]] ## variable bin count count_distr good bad badprob woe bin_iv total_iv breaks ## 1: other.installment.plans bank%,%stores 186 0.186 110 76 0.4086 0.4776 0.04594 0.05759 bank%,%stores ## 2: other.installment.plans none 814 0.814 590 224 0.2752 -0.1212 0.01166 0.05759 none ## is_special_values ## 1: FALSE ## 2: FALSE woebin_plot(bins[[12]]) ## $other.installment.plans # converting train and test into woe values dt_woe_list = lapply(dt_list, function(x) woebin_ply(x, bins)) ## [INFO] converting into woe values ...  模型拟合 当获得了 woe 值替换之后的数据集，可以使用逻辑回归进行拟合，并通过AIC、LASSO等方法对变量进一步筛选。下面使用基于 AIC 的逐步回归进一步筛选变量，最终得到了一个拥有13个变量的模型。\n# lr m1 = glm( creditability ~ ., family = binomial(), data = dt_woe_list$train) # vif(m1, merge_coef = TRUE) # summary(m1) # Select a formula-based model by AIC (or by LASSO for large dataset) m_step = step(m1, direction=\u0026quot;both\u0026quot;, trace = FALSE) m2 = eval(m_step$call) vif(m2, merge_coef = TRUE) # summary(m2) ## variable Estimate Std. Error z value Pr(\u0026gt;|z|) gvif ## 1: (Intercept) -0.9448 0.1094 -8.639 0.0000 NA ## 2: status.of.existing.checking.account_woe 0.7756 0.1380 5.619 0.0000 1.042 ## 3: duration.in.month_woe 0.7963 0.2291 3.476 0.0005 1.181 ## 4: credit.history_woe 0.8308 0.2035 4.082 0.0000 1.064 ## 5: purpose_woe 0.8632 0.2755 3.133 0.0017 1.043 ## 6: credit.amount_woe 0.7669 0.2838 2.702 0.0069 1.251 ## 7: savings.account.and.bonds_woe 0.8545 0.2606 3.279 0.0010 1.039 ## 8: installment.rate.in.percentage.of.disposable.income_woe 1.8621 0.6822 2.730 0.0063 1.094 ## 9: other.debtors.or.guarantors_woe 2.1018 0.8922 2.356 0.0185 1.037 ## 10: age.in.years_woe 1.0154 0.3001 3.383 0.0007 1.033 ## 11: other.installment.plans_woe 0.7623 0.4347 1.754 0.0795 1.060 ## 12: housing_woe 0.7610 0.3665 2.077 0.0378 1.035  模型评估 通过逻辑回归获得各变量的拟合系数之后，可以计算出各个样本为坏客户的概率，然后评估模型的预测效果。 perf_eva 函数能够计算的评估指标包括 mse, rmse, logloss, r2, ks, auc, gini，以及绘制多种可视化图形 ks, lift, gain, roc, lz, pr, f1, density。\n## predicted proability pred_list = lapply(dt_woe_list, function(x) predict(m2, x, type=\u0026#39;response\u0026#39;)) ## performance perf = perf_eva(pred = pred_list, label = label_list) ## [INFO] The threshold of confusion matrix is 0.3133.  评分卡刻度 当我们获得了各个变量的分箱结果，并且确定了最终进入模型的变量以及系数，则可以创建标准评分卡。\n有了评分卡之后，可用于对新样本进行打分，从而评估该客户的信用水平，并最终作出审批决策。\n最后，评分卡模型的开发过程，还需要对模型的稳定性进行评估，即计算psi。\n## scorecard card = scorecard(bins, m2) ## credit score score_list = lapply(dt_list, function(x) scorecard_ply(x, card)) ## psi perf_psi(score = score_list, label = label_list) ## $psi ## Null data.table (0 rows and 0 cols) 以上代码均可以在该项目的主页获取。\n "
},
{
	"uri": "/post/stringr_regex/",
	"title": "stringr 与 regex 函数对应关系",
	"tags": ["R"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "stringr 是 Hadley 大神写的 tidyverse1 系列数据处理包中专门用于处理文本数据的，其函数命名统一易于记忆。而 R 基础包中的文本处理函数 (regex) 的命名规则不是特别统一。下表给出了主要函数之间的映射关系，便于以后查看2。\n   stringr包中函数 功能说明 R Base 中对应函数     使用正则表达式的函数     str_extract() 提取首个匹配模式的字符 regmatches()   str_extract_all() 提取所有匹配模式的字符 regmatches()   str_locate() 返回首个匹配模式的字符的位置 regexpr()   str_locate_all() 返回所有匹配模式的字符的位置 gregexpr()   str_replace() 替换首个匹配模式 sub()   str_replace_all() 替换所有匹配模式 gsub()   str_split() 按照模式分割字符串 strsplit()   str_split_fixed() 按照模式将字符串分割成指定个数 -   str_detect() 检测字符是否存在某些指定模式 grepl()   str_count() 返回指定模式出现的次数 -   其他重要函数     str_sub() 提取指定位置的字符 regmatches()   str_dup() 丢弃指定位置的字符 -   str_length() 返回字符的长度 nchar()   str_pad() 填补字符 -   str_trim() 丢弃填充，如去掉字符前后的空格 -   str_c() 连接字符 paste(),paste0()     r 与 python 中三个数据处理系列包：data.table、tidyverse、pandas \u0026#x21a9;\u0026#xfe0e;\n stringr 网站也给出了两者的映射关系 \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "/post/markdown/",
	"title": "Markdown相关资料",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": " John Gruber's Markdown syntax (中文翻译) John MacFarlane's Pandoc Markdown (中文翻译) Blackfriday Markdown (github) RMarkdown (github) MathJax (中文版)  "
},
{
	"uri": "/tags/invest/",
	"title": "invest",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "/post/ubuntu-vnpy/",
	"title": "在ubuntu上配置vnpy",
	"tags": ["invest"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "vnpy是基于python的中文开源交易平台开发框架，该项目相关信息参见github主页、官方主页、项目知乎专栏、官方社区维恩派、作者知乎主页。\n目前CTP只提供Windows和Linux的接口，所以在ubuntu上配置vnpy^[详细参考vn.trader的Ubuntu运行环境搭建教程]，主要包括三步，**1.**安装python环境anaconda2，**2.**安装mongodb与相关依赖工具，**3.**安装vnpy框架\n安装anaconda 出于速度方面的考虑，目前vnpy官方建议使用python2.*环境。从continuum官方网站下载Python 2.7版Linux 64位的anaconda安装文件，我这里下载了anaconda2-4.3.1（官方建议下载4.0版本）。然后在terminal中cd到下载的anaconda文件夹，运行如下命令:\n1  bash Anaconda2-4.3.1-Linux-x86_64.sh   安装mongodb 在ubuntu中安装mongodb非常简单，在terminal中输入如下命令:\n1  sudo apt-get install mongodb   然后安装pymongo包与Qt黑色主题\n1  conda install pymongo qdarkstyle   以及编译API相关的工具\n1  sudo apt-get install git build-essential libboost-all-dev python-dev cmake   安装vnpy框架 最后从github上下载vnpy项目库。我先Fork了vnpy的GitHub项目库，然后clone到本地：\n1  git clone git@github.com:shichenxie/vnpy.git   在terminal中打开vn.trader文件夹(cd vnpy/vn.trader)，运行python vtMain.py。但是出现缺少PyQt4包的错误(ImportError: No module named PyQt4.QtCore)。原来新版anaconda已默认集成pyqt5^[参考vn.py安装后无法打开，提示No module named PyQt4.QtCore]，解决方法是安装anaconda4.0版本或者安装需要pyqt4的python包(conda install pyqtgraph)\n再次运行python vtMain.py，出现缺少talib包错误，解决方法^[参考ubuntu下面安装ta-lib库]conda install -chttps://conda.anaconda.org/quantopian ta-lib\n"
},
{
	"uri": "/post/new-site/",
	"title": "博客搭建过程",
	"tags": ["R"],
	"categories": [],
	"series": [],
	"description": "",
	"content": "在“我网故我在”的召唤下，我使用 R 语言的 blogdown 包1 和 GitHubPages 在一小时内搭建了本博客。搭建过程分为三个步骤：编辑网站文件、创建 GitHub Pages 仓库、域名绑定。\n编辑网站文件 首先需要编写网站文件，也就是一堆 HTML、JS、CSS 文件。益辉的 blogdown 让静态网站文件编写简单到了一条 R 语句。在编辑网站文件之前，最好用 RStudio 新建一个空的项目文件夹，便于文件管理。在 R 中敲入如下代码\n# 安装blogdown包 devtools::install_github('rstudio/blogdown') setwd(path) # path为新建的项目文件夹路径 # 或者直接用rstudio打开*.Rproj文件 # 创建网站文件 blogdown::new_site() # 默认主题 theme = \u0026quot;yihui/hugo-lithium\u0026quot; # \u0026gt; sessionInfo() # R语言系统环境 # R version 3.3.2 (2016-10-31) # Platform: x86_64-apple-darwin13.4.0 (64-bit) # Running under: macOS Sierra 10.12.3 创建GitHub Pages仓库 登录自己的 github 主页（例如我的主页 https://github.com/shichenxie，其中shichenxie为我的 github 账号），新建名为shichenxie.github.io的项目仓库 (repository)。\n然后将 blogdown 创建的 public 文件夹上传到 github pages 文件夹中。在 terminal 中敲入如下代码2\ncd path # path为public文件地址 git init # 初始化git git remote add origin git@github.com:shichenxie/shichenxie.github.io.git # shichenxie 为我的用户名 git add -A # git中添加所有文件 git commit -m 'init site' # commit git push -u origin master # push 绑定域名 首先在 public 文件夹添加名为 CNAME 的文件（无任何后缀名），然后用文本编辑器打开，写入网站域名 (我的网站域名为 shichen.name)。上传 CNAME 文件到 github 之后，打开shichenxie.github.io自动定位到shichen.name，但是并不能显示网站内容，需要设置 DNS 解析。\n我的网站域名是从阿里云购买的，阿里云提供了的云解析 DNS 服务(也可从 GoDaddy 购买，从 DNSPOD 获取 DNS 解析服务)，添加如下三条记录：3\n@ A 192.30.252.153 @ A 192.30.252.154 www CNAME username.github.io. 最后在浏览器中打开 shichen.name，铛铛\u0026hellip; 个人网站上线了。\n blogdown: Creating Websites with R Markdown \u0026#x21a9;\u0026#xfe0e;\n GitHub Pages 静态博客 - 个人建站实录 \u0026#x21a9;\u0026#xfe0e;\n Using a custom domain with GitHub Pages与域名 CNAME 解析设置方法 \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "/about/",
	"title": "About",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "About Page",
	"content": "谢士晨，汉族人，乙丑年生于江西玉山。2014年底毕业于名古屋大学，获得国际开发学博士学位，研究方向为能源环境经济学。毕业之后在北京从事数据分析师工作。\n本博客是基于 blogdown包 调用 hugo 静态网站的 zzo 主题，部署在 GitHub Pages。搭建博客是为了记录与分享在数据科学相关领域的学习与研究。\n"
},
{
	"uri": "/donate/",
	"title": "",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": " alipay  wechatpay  paypal https://www.paypal.me/shichenxie\n "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
},
{
	"uri": "/slide/",
	"title": "Presentations",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "Presentation list with reveal.js",
	"content": ""
},
{
	"uri": "/project/",
	"title": "Project",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "Project page",
	"content": "1. R 包 scorecard, 2017     提高了评分卡模型的开发效率，提供了特征预处理、变量初筛、最优分箱、模型评估、评分刻度转换与模型报告输出等多个功能。  2. python 包 scorecardpy, 2018     该包是 python 版本的 R 包 scorecard。提高了用 python 开发评分卡模型的效率，提供了变量初筛、最优分箱、模型评估与评分刻度等多个功能。  3. R 包 pedquant, 2019     pedquant 能够获取国家统计局 NBS、美联储财经数据 FRED、全球股票、国内期货等财经数据，并提供了可视化与量化分析等功能。  "
},
{
	"uri": "/series/",
	"title": "Series",
	"tags": [],
	"categories": [],
	"series": [],
	"description": "",
	"content": ""
}]